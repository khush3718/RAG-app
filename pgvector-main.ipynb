{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import PyPDF2\n",
    "import os\n",
    "\n",
    "def read_text_file(file_path: str):\n",
    "    \"\"\"Read content from a text file\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def read_pdf_file(file_path: str):\n",
    "    \"\"\"Read content from a PDF file\"\"\"\n",
    "    text = \"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def read_docx_file(file_path: str):\n",
    "    \"\"\"Read content from a Word document\"\"\"\n",
    "    doc = docx.Document(file_path)\n",
    "    return \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_document(file_path: str):\n",
    "    \"\"\"Read document content based on file extension\"\"\"\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "    file_extension = file_extension.lower()\n",
    "\n",
    "    if file_extension == '.txt':\n",
    "        return read_text_file(file_path)\n",
    "    elif file_extension == '.pdf':\n",
    "        return read_pdf_file(file_path)\n",
    "    elif file_extension == '.docx':\n",
    "        return read_docx_file(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_extension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text: str, chunk_size: int = 500):\n",
    "    \"\"\"Split text into chunks while preserving sentence boundaries\"\"\"\n",
    "    sentences = text.replace('\\n', ' ').split('. ')\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "\n",
    "        # Ensure proper sentence ending\n",
    "        if not sentence.endswith('.'):\n",
    "            sentence += '.'\n",
    "\n",
    "        sentence_size = len(sentence)\n",
    "\n",
    "        # Check if adding this sentence would exceed chunk size\n",
    "        if current_size + sentence_size > chunk_size and current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_size = sentence_size\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_size += sentence_size\n",
    "\n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create table in vector_db in postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TABLE documents (\n",
    "#     id SERIAL PRIMARY KEY,\n",
    "#     document_id TEXT,\n",
    "#     chunk_id INT,\n",
    "#     content TEXT,\n",
    "#     embedding VECTOR(1536)  -- Adjust dimensions to match embedding size\n",
    "# );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from pgvector.psycopg2 import register_vector\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DB_NAME= os.environ['DB_NAME']\n",
    "DB_PORT= os.environ['DB_PORT']\n",
    "DB_HOST= os.environ['DB_HOST']\n",
    "DB_PASSWORD= os.environ['DB_PASSWORD']\n",
    "DB_USER= os.environ['DB_USER']\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "connection = psycopg2.connect(\n",
    "    dbname=DB_NAME,\n",
    "    user=DB_USER,\n",
    "    password=DB_PASSWORD,\n",
    "    host=DB_HOST,\n",
    "    port=DB_PORT\n",
    ")\n",
    "register_vector(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_document_chunk(document_id, chunk_id, content, embedding):\n",
    "    \"\"\"Insert a document chunk and its embedding into PostgreSQL.\"\"\"\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(\n",
    "            \"INSERT INTO documents (document_id, chunk_id, content, embedding) VALUES (%s, %s, %s, %s)\",\n",
    "            (document_id, chunk_id, content, embedding)\n",
    "        )\n",
    "    connection.commit()\n",
    "\n",
    "\n",
    "\n",
    "def get_embedding(text):\n",
    "    \"\"\"Generate embedding for a given text.\"\"\"\n",
    "    return embedding_model.encode(text).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_store_document(file_path: str):\n",
    "    \"\"\"Process a document, generate embeddings, and store them in PostgreSQL.\"\"\"\n",
    "    content = read_document(file_path)\n",
    "    chunks = split_text(content)\n",
    "\n",
    "    document_id = os.path.basename(file_path)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        embedding = get_embedding(chunk)\n",
    "        insert_document_chunk(document_id, i, chunk, embedding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search_pg(query: str, n_results: int = 2):\n",
    "    \"\"\"Perform semantic search on the PostgreSQL collection with pgvector embeddings.\"\"\"\n",
    "    embedding = embedding_model.encode(query).tolist()\n",
    "    # query_vector = f\"ARRAY{embedding}\"\n",
    "\n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            # Using the embedding with the vector <-> operator in PostgreSQL\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                SELECT content, document_id, chunk_id\n",
    "                FROM documents\n",
    "                ORDER BY embedding <-> %s::VECTOR(768)\n",
    "                LIMIT %s;\n",
    "                \"\"\",\n",
    "                (embedding, n_results)\n",
    "            )\n",
    "            results = cursor.fetchall()\n",
    "\n",
    "            # Commit if query succeeds\n",
    "            connection.commit()\n",
    "\n",
    "            # Process results for context and metadata\n",
    "            documents = [row[0] for row in results]\n",
    "            metadatas = [{\"source\": row[1], \"chunk\": row[2]} for row in results]\n",
    "\n",
    "            return {\"documents\": [documents], \"metadatas\": [metadatas]}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during semantic search: {e}\")\n",
    "        connection.rollback()  # Rollback if there's an error\n",
    "        return {\"documents\": [[]], \"metadatas\": [[]]}  # Return empty structure on error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"E:\\Coding\\python/rag\\docs\"\n",
    "for file_name in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    process_and_store_document(file_path)\n",
    "\n",
    "def get_context_with_sources_pg(results):\n",
    "    \"\"\"Format context and source information from PostgreSQL search results.\"\"\"\n",
    "    context = \"\\n\\n\".join([row[2] for row in results])  # Extract content\n",
    "    sources = [f\"{row[0]} (chunk {row[1]})\" for row in results]  # Format source info\n",
    "    return context, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_with_sources_pg(results):\n",
    "    \"\"\"Format context and source information from PostgreSQL search results.\"\"\"\n",
    "    context = \"\\n\\n\".join([row[2] for row in results])  # Extract content\n",
    "    sources = [f\"{row[0]} (chunk {row[1]})\" for row in results]  # Format source info\n",
    "    return context, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(context: str, conversation_history: str, query: str):\n",
    "    \"\"\"Generate a prompt combining context, history, and query\"\"\"\n",
    "    prompt = f\"\"\"Based on the following context and conversation history, \n",
    "    please provide a relevant and contextual response.Look through every part of the document like tables if they exists and give answers based on that. If the answer cannot \n",
    "    be derived from the context, only use the conversation history or say \n",
    "    \"I cannot answer this based on the provided information.\"\n",
    "\n",
    "    Context from documents:\n",
    "    {context}\n",
    "\n",
    "    Previous conversation:\n",
    "    {conversation_history}\n",
    "\n",
    "    Human: {query}\n",
    "\n",
    "    Assistant:\"\"\"\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query: str, context: str, conversation_history: str = \"\"):\n",
    "    \"\"\"Generate a response using Gemini with a dynamic prompt and configurable generation parameters.\"\"\"\n",
    "    prompt = get_prompt(context, conversation_history, query)\n",
    "\n",
    "    try:\n",
    "        # Initialize the model\n",
    "        model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "        \n",
    "        # Generate response using the prompt with a customized generation config\n",
    "        response = model.generate_content(\n",
    "            prompt,  # Use the dynamically generated prompt here\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                candidate_count=1,  # Generates one response candidate\n",
    "                # stop_sequences=[\"\\n\",\"End of answer\"],  # Adjust stop sequences as needed\n",
    "                max_output_tokens=800,  # Set your desired max tokens\n",
    "                temperature=0  # Adjust temperature for response variability\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Extract the response content\n",
    "        return response.text if response else \"No content generated.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error generating response: {str(e)}\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: what are data structures?\n",
      "Answer: Data structures are ways of organizing and storing data in a computer so that it can be accessed and used efficiently. They provide a framework for managing data, allowing for operations like searching, sorting, and inserting new data. \n",
      "\n",
      "Here are some common examples of data structures:\n",
      "\n",
      "* **Arrays:** A collection of elements of the same data type stored in contiguous memory locations.\n",
      "* **Linked Lists:** A linear data structure where elements are linked together using pointers.\n",
      "* **Stacks:** A LIFO (Last-In, First-Out) data structure where elements are added and removed from the top.\n",
      "* **Queues:** A FIFO (First-In, First-Out) data structure where elements are added at the rear and removed from the front.\n",
      "* **Trees:** A hierarchical data structure where elements are organized in a parent-child relationship.\n",
      "* **Graphs:** A non-linear data structure consisting of nodes (vertices) connected by edges.\n",
      "\n",
      "The choice of data structure depends on the specific requirements of the application, such as the type of data, the operations that need to be performed, and the efficiency considerations. \n",
      "\n",
      "\n",
      "Question: What is a stack data structure?\n",
      "Answer: A stack is a linear data structure that follows the Last-In, First-Out (LIFO) principle. This means that the last element added to the stack is the first one to be removed. Think of it like a stack of plates: you can only add or remove plates from the top.\n",
      "\n",
      "Here are some key operations associated with stacks:\n",
      "\n",
      "* **Push:** Adds an element to the top of the stack.\n",
      "* **Pop:** Removes and returns the element at the top of the stack.\n",
      "* **Peek:** Returns the element at the top of the stack without removing it.\n",
      "* **IsEmpty:** Checks if the stack is empty.\n",
      "* **IsFull:** Checks if the stack is full (if it has a fixed size).\n",
      "\n",
      "Stacks are used in various applications, including:\n",
      "\n",
      "* **Function call stack:**  Keeps track of active function calls.\n",
      "* **Undo/Redo functionality:** Stores actions for undoing or redoing operations.\n",
      "* **Expression evaluation:** Used to evaluate mathematical expressions.\n",
      "* **Backtracking algorithms:**  Used to explore different possibilities in a problem. \n",
      "\n",
      "\n",
      "Question: what is case processing in pharmacovigilance?\n",
      "Answer: I cannot answer this based on the provided information. The context provided is simply the letter \"c\" and \"t\".  Please provide more context or information about case processing in pharmacovigilance. \n",
      "\n",
      "\n",
      "Question: what is the role of Mr. Ramesh patel in the leadership team?\n",
      "Answer: I cannot answer this based on the provided information. Please provide me with the context about Mr. Ramesh Patel and the leadership team. \n",
      "\n",
      "\n",
      "Question: what is the budget strategy of adrta?\n",
      "Answer: I cannot answer this based on the provided information. The context provided is simply the letter \"c\" and \"t\".  Please provide more information about ADRTA and their budget strategy. \n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "Sources used for each question:\n",
      "\n",
      "Question: what are data structures?\n",
      "- d (chunk o)\n",
      "- m (chunk e)\n",
      "\n",
      "Question: What is a stack data structure?\n",
      "- d (chunk o)\n",
      "- m (chunk e)\n",
      "\n",
      "Question: what is case processing in pharmacovigilance?\n",
      "- d (chunk o)\n",
      "- m (chunk e)\n",
      "\n",
      "Question: what is the role of Mr. Ramesh patel in the leadership team?\n",
      "- d (chunk o)\n",
      "- m (chunk e)\n",
      "\n",
      "Question: what is the budget strategy of adrta?\n",
      "- d (chunk o)\n",
      "- m (chunk e)\n"
     ]
    }
   ],
   "source": [
    "def rag_query_pg(questions: list, n_chunks: int = 2):\n",
    "    \"\"\"Perform RAG query for a list of questions: retrieve relevant chunks and generate answers.\"\"\"\n",
    "    responses = []\n",
    "    sources_used = []\n",
    "    \n",
    "    for question in questions:\n",
    "        try:\n",
    "            # Get relevant chunks for each question\n",
    "            results = semantic_search_pg(question, n_chunks)\n",
    "            context, sources = get_context_with_sources_pg(results)\n",
    "            \n",
    "            # Generate response for each question\n",
    "            response = generate_response(question, context)\n",
    "            \n",
    "            # Append results for this question\n",
    "            responses.append((question, response))\n",
    "            sources_used.append((question, sources))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question '{question}': {e}\")\n",
    "            connection.rollback()  # Rollback transaction if there's an error\n",
    "    \n",
    "    return responses, sources_used\n",
    "\n",
    "\n",
    "questions = [\n",
    "    \"what are data structures?\",\n",
    "    \"What is a stack data structure?\",\n",
    "    # \"How has cloud computing evolved in the past decade?\",\n",
    "    \"what is case processing in pharmacovigilance?\",\n",
    "    \"what is the role of Mr. Ramesh patel in the leadership team?\",\n",
    "    \"what is the budget strategy of adrta?\"\n",
    "]\n",
    "responses, sources = rag_query_pg(questions)\n",
    "\n",
    "# Print results\n",
    "for question, response in responses:\n",
    "    print(\"\\nQuestion:\", question)\n",
    "    print(\"Answer:\", response)\n",
    "\n",
    "print(\"\\n------------------------------------------\")\n",
    "print(\"\\nSources used for each question:\")\n",
    "for question, source_list in sources:\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    for source in source_list:\n",
    "        print(f\"- {source}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
