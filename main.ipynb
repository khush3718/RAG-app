{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import PyPDF2\n",
    "import os\n",
    "\n",
    "def read_text_file(file_path: str):\n",
    "    \"\"\"Read content from a text file\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def read_pdf_file(file_path: str):\n",
    "    \"\"\"Read content from a PDF file\"\"\"\n",
    "    text = \"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def read_docx_file(file_path: str):\n",
    "    \"\"\"Read content from a Word document\"\"\"\n",
    "    doc = docx.Document(file_path)\n",
    "    return \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating a unified interface for document reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_document(file_path: str):\n",
    "    \"\"\"Read document content based on file extension\"\"\"\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "    file_extension = file_extension.lower()\n",
    "\n",
    "    if file_extension == '.txt':\n",
    "        return read_text_file(file_path)\n",
    "    elif file_extension == '.pdf':\n",
    "        return read_pdf_file(file_path)\n",
    "    elif file_extension == '.docx':\n",
    "        return read_docx_file(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_extension}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Chunking process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text: str, chunk_size: int = 500):\n",
    "    \"\"\"Split text into chunks while preserving sentence boundaries\"\"\"\n",
    "    sentences = text.replace('\\n', ' ').split('. ')\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "\n",
    "        # Ensure proper sentence ending\n",
    "        if not sentence.endswith('.'):\n",
    "            sentence += '.'\n",
    "\n",
    "        sentence_size = len(sentence)\n",
    "\n",
    "        # Check if adding this sentence would exceed chunk size\n",
    "        if current_size + sentence_size > chunk_size and current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_size = sentence_size\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_size += sentence_size\n",
    "\n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# improved chunking process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from typing import List\n",
    "\n",
    "def split_text(text: str, chunk_size: int = 500, overlap: int = 50, token_based: bool = False) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into chunks while preserving sentence and paragraph boundaries, \n",
    "    with optional overlap and token-based chunking.\n",
    "    \"\"\"\n",
    "    # Load nltk sentence tokenizer if needed\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    \n",
    "    # Tokenize by paragraphs first\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "\n",
    "    def get_token_count(text):\n",
    "        \"\"\"Helper to get the token count (rough approximation).\"\"\"\n",
    "        return len(re.findall(r'\\w+', text))\n",
    "\n",
    "    # Adjust sentence or token counting function\n",
    "    count_func = get_token_count if token_based else len\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        sentences = nltk.sent_tokenize(paragraph.strip())\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "\n",
    "            sentence_size = count_func(sentence)\n",
    "            \n",
    "            # Add sentence to current chunk if it fits\n",
    "            if current_size + sentence_size <= chunk_size:\n",
    "                current_chunk.append(sentence)\n",
    "                current_size += sentence_size\n",
    "            else:\n",
    "                # Finalize the current chunk\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                \n",
    "                # Begin new chunk, optionally with overlap from last few sentences\n",
    "                if overlap > 0 and len(current_chunk) > 0:\n",
    "                    overlap_sentences = current_chunk[-overlap:]\n",
    "                    current_chunk = overlap_sentences + [sentence]\n",
    "                    current_size = count_func(' '.join(current_chunk))\n",
    "                else:\n",
    "                    current_chunk = [sentence]\n",
    "                    current_size = sentence_size\n",
    "\n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting up ChromaDB "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using sentence transformers embeddings to initialize chromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Coding\\python\\rag\\rag-env\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Initialize ChromaDB client with persistence\n",
    "client = chromadb.PersistentClient(path=\"chroma_db\")\n",
    "\n",
    "# Configure sentence transformer embeddings\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Create or get existing collection\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"documents_collection\",\n",
    "    embedding_function=sentence_transformer_ef\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inserting data into ChromaDB \n",
    "\n",
    "preparing a pipeline that processes documents and prepares them for insertion in chromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(file_path: str):\n",
    "    \"\"\"Process a single document and prepare it for ChromaDB\"\"\"\n",
    "    try:\n",
    "        # Read the document\n",
    "        content = read_document(file_path)\n",
    "\n",
    "        # Split into chunks\n",
    "        chunks = split_text(content)\n",
    "\n",
    "        # Prepare metadata\n",
    "        file_name = os.path.basename(file_path)\n",
    "        metadatas = [{\"source\": file_name, \"chunk\": i} for i in range(len(chunks))]\n",
    "        ids = [f\"{file_name}_chunk_{i}\" for i in range(len(chunks))]\n",
    "\n",
    "        return ids, chunks, metadatas\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return [], [], []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batch processing for multiple documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_collection(collection, ids, texts, metadatas):\n",
    "    \"\"\"Add documents to collection in batches\"\"\"\n",
    "    if not texts:\n",
    "        return\n",
    "\n",
    "    batch_size = 100\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        end_idx = min(i + batch_size, len(texts))\n",
    "        collection.add(\n",
    "            documents=texts[i:end_idx],\n",
    "            metadatas=metadatas[i:end_idx],\n",
    "            ids=ids[i:end_idx]\n",
    "        )\n",
    "\n",
    "def process_and_add_documents(collection, folder_path: str):\n",
    "    \"\"\"Process all documents in a folder and add to collection\"\"\"\n",
    "    files = [os.path.join(folder_path, file) \n",
    "             for file in os.listdir(folder_path) \n",
    "             if os.path.isfile(os.path.join(folder_path, file))]\n",
    "\n",
    "    for file_path in files:\n",
    "        print(f\"Processing {os.path.basename(file_path)}...\")\n",
    "        ids, texts, metadatas = process_document(file_path)\n",
    "        add_to_collection(collection, ids, texts, metadatas)\n",
    "        print(f\"Added {len(texts)} chunks to collection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\khush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trying chromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2180712_CIS_GTU_Study_Material_e-Notes_All-Units_17062020050424AM.pdf...\n",
      "Added 1533 chunks to collection\n",
      "Processing 3140705_OOP---I_GTU_Study_Material_e-Notes_Unit-1-to-5_11062022015400PM (1).pdf...\n",
      "Added 1536 chunks to collection\n",
      "Processing e-Notes_PDF_All-Units_24042019090707AM.pdf...\n",
      "Added 1101 chunks to collection\n",
      "Processing Services_Proposal Document - Adrta.docx...\n",
      "Added 106 chunks to collection\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChromaDB collection \n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"documents_collection\",\n",
    "    embedding_function=sentence_transformer_ef\n",
    ")\n",
    "\n",
    "# Process and add documents from a folder\n",
    "folder_path = \"E:\\Coding\\python/rag\\docs\"\n",
    "process_and_add_documents(collection, folder_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# semantic search to retreive relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(collection, query: str, n_results: int = 2):\n",
    "    \"\"\"Perform semantic search on the collection\"\"\"\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    return results\n",
    "\n",
    "def get_context_with_sources(results):\n",
    "    \"\"\"Extract context and source information from search results\"\"\"\n",
    "    # Combine document chunks into a single context\n",
    "    context = \"\\n\\n\".join(results['documents'][0])\n",
    "\n",
    "    # Format sources with metadata\n",
    "    sources = [\n",
    "        f\"{meta['source']} (chunk {meta['chunk']})\" \n",
    "        for meta in results['metadatas'][0]\n",
    "    ]\n",
    "\n",
    "    return context, sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search Results:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Result 1\n",
      "Source: 3140705_OOP---I_GTU_Study_Material_e-Notes_Unit-1-to-5_11062022015400PM (1).pdf, Chunk 3\n",
      "Distance: 0.4537600576877594\n",
      "Content:  On 8 May 2007, Sun finished the process, making all of Java's core code free and open -source,  aside from a small portion of code to which Sun did not hold the copyright. What is Java?    Java is a programming language that:    Is exclusively object oriented    Has full GUI support    Has full network support    Is platform independent    Executes stand -alone or “on -demand” in web browser as applets      1 - Basics of Java      2    Prof. Swati R.\n",
      "\n",
      "\n",
      "Result 2\n",
      "Source: 3140705_OOP---I_GTU_Study_Material_e-Notes_Unit-1-to-5_11062022015400PM (1).pdf, Chunk 4\n",
      "Distance: 0.5449849367141724\n",
      "Content: Sharma  | 3140705  – Object Oriented Programming - I JDK, JRE , Byte  code  & JVM.  Java Development Kit (JDK)   o JDK contains tools needed ,    To develop the Java programs and    JRE to run the programs. o The tools include compiler (javac.exe), Java application launcher (java.exe),  Appletviewer, etc…   o Java application launcher (java.exe),    Opens a JRE, loads the class, and invokes its main method.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform a search\n",
    "query = \"what is java?\"\n",
    "results = semantic_search(collection, query)\n",
    "# results\n",
    "\n",
    "def print_search_results(results):\n",
    "    \"\"\"Print formatted search results\"\"\"\n",
    "    print(\"\\nSearch Results:\\n\" + \"-\" * 100)\n",
    "\n",
    "    for i in range(len(results['documents'][0])):\n",
    "        doc = results['documents'][0][i]\n",
    "        meta = results['metadatas'][0][i]\n",
    "        distance = results['distances'][0][i]\n",
    "\n",
    "        print(f\"\\nResult {i + 1}\")\n",
    "        print(f\"Source: {meta['source']}, Chunk {meta['chunk']}\")\n",
    "        print(f\"Distance: {distance}\")\n",
    "        print(f\"Content: {doc}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "print_search_results(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "configuring gemini model to generate answers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(context: str, conversation_history: str, query: str):\n",
    "    \"\"\"Generate a prompt combining context, history, and query\"\"\"\n",
    "    prompt = f\"\"\"Based on the following context and conversation history, \n",
    "    please provide a relevant and contextual response.Look through every part of the document like tables if they exists and give answers based on that. If the answer cannot \n",
    "    be derived from the context, only use the conversation history or say \n",
    "    \"I cannot answer this based on the provided information.\"\n",
    "\n",
    "    Context from documents:\n",
    "    {context}\n",
    "\n",
    "    Previous conversation:\n",
    "    {conversation_history}\n",
    "\n",
    "    Human: {query}\n",
    "\n",
    "    Assistant:\"\"\"\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# configuring model to answer queries/questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query: str, context: str, conversation_history: str = \"\"):\n",
    "    \"\"\"Generate a response using Gemini with a dynamic prompt and configurable generation parameters.\"\"\"\n",
    "    prompt = get_prompt(context, conversation_history, query)\n",
    "\n",
    "    try:\n",
    "        # Initialize the model\n",
    "        model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "        \n",
    "        # Generate response using the prompt with a customized generation config\n",
    "        response = model.generate_content(\n",
    "            prompt,  # Use the dynamically generated prompt here\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                candidate_count=1,  # Generates one response candidate\n",
    "                # stop_sequences=[\"\\n\",\"End of answer\"],  # Adjust stop sequences as needed\n",
    "                max_output_tokens=800,  # Set your desired max tokens\n",
    "                temperature=0.2  # Adjust temperature for response variability\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Extract the response content\n",
    "        return response.text if response else \"No content generated.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error generating response: {str(e)}\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for a single query at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(collection, query: str, n_chunks: int = 2):\n",
    "    \"\"\"Perform RAG query: retrieve relevant chunks and generate answer\"\"\"\n",
    "    # Get relevant chunks\n",
    "    results = semantic_search(collection, query, n_chunks)\n",
    "    context, sources = get_context_with_sources(results)\n",
    "\n",
    "    # Generate response\n",
    "    response = generate_response(query, context)\n",
    "\n",
    "    return response, sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"give a breif about history of cloud computing\"\n",
    "response, sources = rag_query(collection, query)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nQuery:\", query)\n",
    "print(\"\\nAnswer:\", response)\n",
    "print(\"\\nSources used:\")\n",
    "for source in sources:\n",
    "    print(f\"- {source}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for a list/quesstionnaire of questions all at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(collection, questions: list, n_chunks: int = 2):\n",
    "    \"\"\"Perform RAG query for a list of questions: retrieve relevant chunks and generate answers.\"\"\"\n",
    "    responses = []\n",
    "    sources_used = []\n",
    "    \n",
    "    for question in questions:\n",
    "        # Get relevant chunks for each question\n",
    "        results = semantic_search(collection, question, n_chunks)\n",
    "        context, sources = get_context_with_sources(results)\n",
    "\n",
    "        # Generate response for each question\n",
    "        response = generate_response(question, context)\n",
    "\n",
    "        # Append results for this question\n",
    "        responses.append((question, response))\n",
    "        sources_used.append((question, sources))\n",
    "\n",
    "    return responses, sources_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: explain data structures in detail with example\n",
      "Answer: Data structures are a way of organizing data in a computer's memory. They define the relationship between individual data elements and how they are stored. Think of it like a filing system for your computer. \n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "* **Representation of Data:**  Data structures determine how data is stored in the computer's memory. This could be in the form of numbers, characters, or even complex objects.\n",
      "* **Accessing Data:** Data structures define how you can access and manipulate the stored data.  You can retrieve specific data elements, modify them, or add new ones.\n",
      "* **Logical Relationships:** Data structures establish the connections between data elements. For example, a list might store items in a specific order, while a tree structure might organize data hierarchically.\n",
      "\n",
      "**Example:**\n",
      "\n",
      "Imagine you're creating a program to manage a library. You need to store information about books, such as their title, author, and genre. You could use a data structure called an **array** to store this information. Each book would be represented as an element in the array, and you could access the information about a specific book by its position in the array.\n",
      "\n",
      "**Why are data structures important?**\n",
      "\n",
      "* **Efficient Data Management:** Data structures allow you to organize and access data efficiently, making your programs faster and more responsive.\n",
      "* **Problem Solving:** They provide a framework for solving complex problems by breaking them down into smaller, manageable parts.\n",
      "* **Code Reusability:** Many common data structures are already implemented in programming languages, allowing you to reuse existing code and save time.\n",
      "\n",
      "Let me know if you'd like to explore specific types of data structures, like arrays, linked lists, trees, or graphs. \n",
      "\n",
      "\n",
      "Question: What is a stack data structure?\n",
      "Answer: A stack is a data structure where insertion and deletion operations are performed at one end only. The insertion operation is called 'PUSH' and the deletion operation is called 'POP'. Stacks are also known as Last In First Out (LIFO) data structures. \n",
      "\n",
      "\n",
      "Question: what is case processing in pharmacovigilance?\n",
      "Answer: Case processing in pharmacovigilance refers to the systematic handling of adverse event (AE) and serious adverse event (SAE) reports. It involves a series of steps to ensure accurate and timely reporting of safety information related to pharmaceutical products. \n",
      "\n",
      "\n",
      "Question: find out who is Mr. Ramesh patel from the table in the leadership section\n",
      "Answer: I cannot answer this based on the provided information. The document does not mention a \"leadership section\" or a table containing information about Mr. Ramesh Patel. \n",
      "\n",
      "\n",
      "Question: what is the budget strategy of adrta?\n",
      "Answer: I cannot answer this based on the provided information. The document does not contain information about ADRTA's budget strategy. \n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "Sources used for each question:\n",
      "\n",
      "Question: explain data structures in detail with example\n",
      "- e-Notes_PDF_All-Units_24042019090707AM.pdf (chunk 7)\n",
      "- e-Notes_PDF_All-Units_24042019090707AM.pdf (chunk 5)\n",
      "\n",
      "Question: What is a stack data structure?\n",
      "- e-Notes_PDF_All-Units_24042019090707AM.pdf (chunk 86)\n",
      "- e-Notes_PDF_All-Units_24042019090707AM.pdf (chunk 82)\n",
      "\n",
      "Question: what is case processing in pharmacovigilance?\n",
      "- Services_Proposal Document - Adrta.docx (chunk 75)\n",
      "- Services_Proposal Document - Adrta.docx (chunk 71)\n",
      "\n",
      "Question: find out who is Mr. Ramesh patel from the table in the leadership section\n",
      "- Services_Proposal Document - Adrta.docx (chunk 90)\n",
      "- Services_Proposal Document - Adrta.docx (chunk 88)\n",
      "\n",
      "Question: what is the budget strategy of adrta?\n",
      "- Services_Proposal Document - Adrta.docx (chunk 70)\n",
      "- Services_Proposal Document - Adrta.docx (chunk 69)\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"explain data structures in detail with example\",\n",
    "    \"What is a stack data structure?\",\n",
    "    # \"How has cloud computing evolved in the past decade?\",\n",
    "    \"what is case processing in pharmacovigilance?\",\n",
    "    \"find out who is Mr. Ramesh patel from the table in the leadership section\",\n",
    "    \"what is the budget strategy of adrta?\"\n",
    "]\n",
    "responses, sources = rag_query(collection, questions)\n",
    "\n",
    "# Print results\n",
    "for question, response in responses:\n",
    "    print(\"\\nQuestion:\", question)\n",
    "    print(\"Answer:\", response)\n",
    "\n",
    "print(\"\\n------------------------------------------\")\n",
    "print(\"\\nSources used for each question:\")\n",
    "for question, source_list in sources:\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    for source in source_list:\n",
    "        print(f\"- {source}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input questions via a pdf/doc/xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rag_query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 66\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     65\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCoding\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mpython/rag\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mquestions-xl.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your file path\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m \u001b[43mprocess_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 51\u001b[0m, in \u001b[0;36mprocess_document\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported file type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Process each question with the RAG app\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m responses, sources \u001b[38;5;241m=\u001b[39m \u001b[43mrag_query\u001b[49m(collection, questions)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m question, response \u001b[38;5;129;01min\u001b[39;00m responses:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rag_query' is not defined"
     ]
    }
   ],
   "source": [
    "import PyPDF2  \n",
    "import docx    \n",
    "import pandas as pd  \n",
    "from pathlib import Path\n",
    "\n",
    "def extract_questions_from_pdf(file_path):\n",
    "    \"\"\"Extract questions from a PDF document using PyPDF2.\"\"\"\n",
    "    questions = []\n",
    "    with open(file_path, \"rb\") as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        for page in pdf_reader.pages:\n",
    "            text = page.extract_text()\n",
    "            questions += [line.strip() for line in text.splitlines() if line.strip().endswith('?')]\n",
    "    return questions\n",
    " \n",
    "def extract_questions_from_docx(file_path):\n",
    "    \"\"\"Extract questions from a Word document.\"\"\"\n",
    "    questions = []\n",
    "    doc = docx.Document(file_path)\n",
    "    for paragraph in doc.paragraphs:\n",
    "        if paragraph.text.strip().endswith('?'):\n",
    "            questions.append(paragraph.text.strip())\n",
    "    return questions\n",
    "\n",
    "def extract_questions_from_excel(file_path):\n",
    "    \"\"\"Extract questions from an Excel document.\"\"\"\n",
    "    questions = []\n",
    "    df = pd.read_excel(file_path)\n",
    "    for col in df.columns:\n",
    "        for value in df[col].dropna():\n",
    "            if isinstance(value, str) and value.strip().endswith('?'):\n",
    "                questions.append(value.strip())\n",
    "    return questions\n",
    "\n",
    "def process_document(file_path):\n",
    "    \"\"\"Automatically detect file type, extract questions, and process them with the RAG pipeline.\"\"\"\n",
    "    # Detect file type\n",
    "    file_extension = Path(file_path).suffix.lower()\n",
    "    \n",
    "    # Map the file extension to the appropriate extraction function\n",
    "    if file_extension == \".pdf\":\n",
    "        questions = extract_questions_from_pdf(file_path)\n",
    "    elif file_extension == \".docx\":\n",
    "        questions = extract_questions_from_docx(file_path)\n",
    "    elif file_extension == \".xlsx\":\n",
    "        questions = extract_questions_from_excel(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type\")\n",
    "    \n",
    "    # Process each question with the RAG app\n",
    "    responses, sources = rag_query(collection, questions)\n",
    "    \n",
    "    # Print results\n",
    "    for question, response in responses:\n",
    "        print(\"\\nQuestion:\", question)\n",
    "        print(\"Answer:\", response)\n",
    "    \n",
    "    print(\"\\nSources used for each question:\")\n",
    "    for question, source_list in sources:\n",
    "        print(f\"\\nQuestion: {question}\")\n",
    "        for source in source_list:\n",
    "            print(f\"- {source}\")\n",
    "\n",
    "# Example usage\n",
    "file_path = \"E:\\Coding\\python/rag\\questions-xl.xlsx\"  # Replace with your file path\n",
    "process_document(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
