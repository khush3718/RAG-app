{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import PyPDF2\n",
    "import os\n",
    "\n",
    "def read_text_file(file_path: str):\n",
    "    \"\"\"Read content from a text file\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def read_pdf_file(file_path: str):\n",
    "    \"\"\"Read content from a PDF file\"\"\"\n",
    "    text = \"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def read_docx_file(file_path: str):\n",
    "    \"\"\"Read content from a Word document\"\"\"\n",
    "    doc = docx.Document(file_path)\n",
    "    return \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_document(file_path: str):\n",
    "    \"\"\"Read document content based on file extension\"\"\"\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "    file_extension = file_extension.lower()\n",
    "\n",
    "    if file_extension == '.txt':\n",
    "        return read_text_file(file_path)\n",
    "    elif file_extension == '.pdf':\n",
    "        return read_pdf_file(file_path)\n",
    "    elif file_extension == '.docx':\n",
    "        return read_docx_file(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_extension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text: str, chunk_size: int = 500):\n",
    "    \"\"\"Split text into chunks while preserving sentence boundaries\"\"\"\n",
    "    sentences = text.replace('\\n', ' ').split('. ')\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "\n",
    "        # Ensure proper sentence ending\n",
    "        if not sentence.endswith('.'):\n",
    "            sentence += '.'\n",
    "\n",
    "        sentence_size = len(sentence)\n",
    "\n",
    "        # Check if adding this sentence would exceed chunk size\n",
    "        if current_size + sentence_size > chunk_size and current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_size = sentence_size\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_size += sentence_size\n",
    "\n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# improved chunking function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from typing import List\n",
    "\n",
    "def split_text(text: str, chunk_size: int = 500, overlap: int = 50, token_based: bool = False) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into chunks while preserving sentence and paragraph boundaries, \n",
    "    with optional overlap and token-based chunking.\n",
    "    \"\"\"\n",
    "    # Load nltk sentence tokenizer if needed\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    \n",
    "    # Tokenize by paragraphs first\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "\n",
    "    def get_token_count(text):\n",
    "        \"\"\"Helper to get the token count (rough approximation).\"\"\"\n",
    "        return len(re.findall(r'\\w+', text))\n",
    "\n",
    "    # Adjust sentence or token counting function\n",
    "    count_func = get_token_count if token_based else len\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        sentences = nltk.sent_tokenize(paragraph.strip())\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "\n",
    "            sentence_size = count_func(sentence)\n",
    "            \n",
    "            # Add sentence to current chunk if it fits\n",
    "            if current_size + sentence_size <= chunk_size:\n",
    "                current_chunk.append(sentence)\n",
    "                current_size += sentence_size\n",
    "            else:\n",
    "                # Finalize the current chunk\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                \n",
    "                # Begin new chunk, optionally with overlap from last few sentences\n",
    "                if overlap > 0 and len(current_chunk) > 0:\n",
    "                    overlap_sentences = current_chunk[-overlap:]\n",
    "                    current_chunk = overlap_sentences + [sentence]\n",
    "                    current_size = count_func(' '.join(current_chunk))\n",
    "                else:\n",
    "                    current_chunk = [sentence]\n",
    "                    current_size = sentence_size\n",
    "\n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Coding\\python\\rag\\rag-env\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "# import google.generativeai as gen_ai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import psycopg2\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "DB_HOST = os.environ['DB_HOST']\n",
    "DB_PORT = os.environ['DB_PORT']\n",
    "DB_NAME = os.environ['DB_NAME']\n",
    "DB_USER = os.environ['DB_USER']\n",
    "DB_PASSWORD = os.environ['DB_PASSWORD']\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=DB_HOST,\n",
    "    port=DB_PORT,\n",
    "    dbname=DB_NAME,\n",
    "    user=DB_USER,\n",
    "    password=DB_PASSWORD\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "def create_embedding_table():\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS document_embeddings (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        document_text TEXT,\n",
    "        embedding VECTOR(768),\n",
    "        source_file TEXT,\n",
    "        chunk_number INT,\n",
    "        UNIQUE(document_text, chunk_number, source_file)  -- Ensure uniqueness\n",
    "    );\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    " \n",
    "create_embedding_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_pgvector(collection_name, chunks, source_file):\n",
    "    embeddings = [model.encode(chunk).tolist() for chunk in chunks]\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        cursor.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO document_embeddings (document_text, embedding, source_file, chunk_number)\n",
    "            VALUES (%s, %s, %s, %s)\n",
    "            \"\"\",\n",
    "            (chunks[i], embedding, source_file, i)\n",
    "        )\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_index_document(file_path):\n",
    "    text = read_document(file_path)\n",
    "    # text = clean_text(text)\n",
    "    chunks = split_text(text)\n",
    "    add_to_pgvector(\"document_collection\", chunks, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_documents_from_folder(folder_path):\n",
    "    files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(('.pdf', '.txt', '.docx'))]\n",
    "    for file_path in files:\n",
    "        print(f\"Processing {os.path.basename(file_path)}...\")\n",
    "        process_and_index_document(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, n_results=2, similarity_threshold=0.7):\n",
    "    query_embedding = model.encode(query).tolist()\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        SELECT document_text, source_file, chunk_number,\n",
    "               (embedding <=> %s::vector(768)) AS similarity\n",
    "        FROM document_embeddings\n",
    "        ORDER BY similarity ASC\n",
    "        LIMIT %s;\n",
    "        \"\"\",\n",
    "        (query_embedding, n_results)\n",
    "    )\n",
    "    results = cursor.fetchall()\n",
    "    relevant_results = [\n",
    "        {\"text\": result[0], \"source\": result[1], \"chunk\": result[2], \"similarity\": result[3]}\n",
    "        for result in results if result[3] <= similarity_threshold\n",
    "    ]\n",
    "    return relevant_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_with_sources(results):\n",
    "    context = \"\\n\\n\".join([result['text'] for result in results])\n",
    "    sources = [f\"{result['source']} (chunk {result['chunk']})\" for result in results]\n",
    "    return context, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_response(query, response, sources):\n",
    "    formatted_sources = \"\\n\".join([f\"- {source}\" for source in sources])\n",
    "    return f\"\"\"Query: {query}\\n\\nAnswer: {response}\\n\\nSources used:\\n{formatted_sources}\\n\"\"\"\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as gen_ai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "gen_ai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context, conversation_history=\"\"):\n",
    "    prompt = f\"\"\"Based on the following context and conversation history,\n",
    "    please provide a relevant and contextual response. If the answer cannot\n",
    "    be derived from the context, only use the conversation history or say\n",
    "    \"I cannot answer this based on the provided information.\"\n",
    " \n",
    "    Context from documents:\n",
    "    {context}\n",
    " \n",
    "    Previous conversation:\n",
    "    {conversation_history}\n",
    " \n",
    "    Human: {query}\n",
    " \n",
    "    Assistant:\"\"\"\n",
    "   \n",
    "    try:\n",
    "        model = gen_ai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=gen_ai.types.GenerationConfig(\n",
    "                candidate_count=1,\n",
    "                # stop_sequences=[\"x\"],\n",
    "                max_output_tokens=900,\n",
    "                temperature=0.2\n",
    "            ),\n",
    "        )\n",
    "        if response and response.text:\n",
    "            return response.text\n",
    "        else:\n",
    "            return \"Unable to generate content due to response restrictions or empty result.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error generating response: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using llama3 model instead of gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def generate_response(query, context, conversation_history=\"\"):\n",
    "    prompt = f\"\"\"Based on the following context and conversation history,\n",
    "    please provide a relevant and contextual response. If the answer cannot\n",
    "    be derived from the context, only use the conversation history or say\n",
    "    \"I cannot answer this based on the provided information.\"\n",
    " \n",
    "    Context from documents:\n",
    "    {context}\n",
    " \n",
    "    Previous conversation:\n",
    "    {conversation_history}\n",
    " \n",
    "    Human: {query}\n",
    " \n",
    "    Assistant:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model=\"llama3\",  # Replace with the specific LLaMA model name if different\n",
    "            prompt=prompt\n",
    "        )\n",
    "        # Extract the response text\n",
    "        if response and \"response\" in response:\n",
    "            return response[\"response\"]\n",
    "        else:\n",
    "            return \"Unable to generate content due to response restrictions or empty result.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error generating response with LLaMA: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(query, n_chunks=2):\n",
    "    results = semantic_search(query, n_chunks)\n",
    "    context, sources = get_context_with_sources(results)\n",
    "    response = generate_response(query, context)\n",
    "    return format_response(query, response, sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multiple_queries(queries, n_chunks=2):\n",
    "    responses = {}\n",
    "    for i, query in enumerate(queries):\n",
    "        formatted_response = rag_query(query, n_chunks)\n",
    "        responses[f\"Question {i+1}\"] = formatted_response\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adding document embeddings in postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2180712_CIS_GTU_Study_Material_e-Notes_All-Units_17062020050424AM.pdf...\n",
      "Processing 3140705_OOP---I_GTU_Study_Material_e-Notes_Unit-1-to-5_11062022015400PM (1).pdf...\n",
      "Processing e-Notes_PDF_All-Units_24042019090707AM.pdf...\n",
      "Processing major project final report.pdf...\n",
      "Processing Services_Proposal Document - Adrta.docx...\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"E:\\Coding\\python/rag\\docs\"\n",
    "add_documents_from_folder(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adding a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"E:\\Coding\\Github desktop\\zentixs_assistant_api\\pdf\\Templates\\services poc.docx\"\n",
    "process_and_index_document(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Please give all the details about introduction and executive summary from the services poc\n",
      "\n",
      "Answer: I cannot provide a response based on the provided information. The context appears to be related to a business proposal or contract, and I do not have enough information to provide an introduction and executive summary for the services proposed. If you'd like to provide more details or clarify what you're looking for, I'll do my best to assist you!\n",
      "\n",
      "Sources used:\n",
      "- E:\\Coding\\python/rag\\docs\\Services_Proposal Document - Adrta.docx (chunk 45)\n",
      "- E:\\Coding\\python/rag\\docs\\Services_Proposal Document - Adrta.docx (chunk 16)\n",
      "\n",
      "Query: give a brief about QPPV and PSMF in pharmacovigilance\n",
      "\n",
      "Answer: Here's a brief overview of QPPV and PSMF in the context of pharmacovigilance:\n",
      "\n",
      "QPPV and PSMF are service delivery records that demonstrate the successful handling of clients with varying needs. This implies that both QPPV and PSMF have a proven track record of adapting to different client requirements, scales, and sizes over a period of time (decade-long relationships). In the context of pharmacovigilance, this suggests that they have successfully processed cases from various sources such as spontaneous reports, literature reviews, regulatory submissions, clinical studies, and legal proceedings. Their case processing activities are well-established to handle intricacies and nuances in these cases.\n",
      "\n",
      "Sources used:\n",
      "- E:\\Coding\\Github desktop\\zentixs_assistant_api\\pdf\\Templates\\services poc.docx (chunk 11)\n",
      "- E:\\Coding\\python/rag\\docs\\Services_Proposal Document - Adrta.docx (chunk 11)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    " \n",
    "queries = [\n",
    "    # \"What are characteristics of cloud computing?\",\n",
    "    # \"Explain arrays in detail.\",\n",
    "    # \"Explain the benefits of CloudTrail.\",\n",
    "    # \"Describe Amazon Simple Storage Service.\",\n",
    "    # \"what are data structures?\",\n",
    "    # \"What is a stack data structure?\",\n",
    "    # \"How has cloud computing evolved in the past decade?\",\n",
    "    # \"what is case processing in pharmacovigilance?\",\n",
    "    # \"what is the role of Mr. Ramesh patel in the leadership team?\",\n",
    "    # \"what is the budget strategy of adrta?\",\n",
    "    # \"What are the various differentiators of ADRTA?\",\n",
    "    # \"how can mental health counselling be refined based on the main topic of major project final report?\",\n",
    "    # \"Summarize the literature review of the major project final report?\",\n",
    "    # \"Who are the authors of the major project final report?\",\n",
    "    # \"What is the scope of the mental health counselling refining project?\"\n",
    "    'Please give all the details about introduction and executive summary from the services poc',\n",
    "    'give a brief about QPPV and PSMF in pharmacovigilance'\n",
    "]\n",
    "responses = process_multiple_queries(queries)\n",
    " \n",
    "# Display formatted results for each question\n",
    "for question, formatted_response in responses.items():\n",
    "    print(formatted_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
