{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import PyPDF2\n",
    "import os\n",
    "\n",
    "def read_text_file(file_path: str):\n",
    "    \"\"\"Read content from a text file\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def read_pdf_file(file_path: str):\n",
    "    \"\"\"Read content from a PDF file\"\"\"\n",
    "    text = \"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def read_docx_file(file_path: str):\n",
    "    \"\"\"Read content from a Word document\"\"\"\n",
    "    doc = docx.Document(file_path)\n",
    "    return \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_document(file_path: str):\n",
    "    \"\"\"Read document content based on file extension\"\"\"\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "    file_extension = file_extension.lower()\n",
    "\n",
    "    if file_extension == '.txt':\n",
    "        return read_text_file(file_path)\n",
    "    elif file_extension == '.pdf':\n",
    "        return read_pdf_file(file_path)\n",
    "    elif file_extension == '.docx':\n",
    "        return read_docx_file(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_extension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text: str, chunk_size: int = 500):\n",
    "    \"\"\"Split text into chunks while preserving sentence boundaries\"\"\"\n",
    "    sentences = text.replace('\\n', ' ').split('. ')\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "\n",
    "        # Ensure proper sentence ending\n",
    "        if not sentence.endswith('.'):\n",
    "            sentence += '.'\n",
    "\n",
    "        sentence_size = len(sentence)\n",
    "\n",
    "        # Check if adding this sentence would exceed chunk size\n",
    "        if current_size + sentence_size > chunk_size and current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_size = sentence_size\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_size += sentence_size\n",
    "\n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# improved chunking function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from typing import List\n",
    "\n",
    "def split_text(text: str, chunk_size: int = 500, overlap: int = 50, token_based: bool = False) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into chunks while preserving sentence and paragraph boundaries, \n",
    "    with optional overlap and token-based chunking.\n",
    "    \"\"\"\n",
    "    # Load nltk sentence tokenizer if needed\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    \n",
    "    # Tokenize by paragraphs first\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "\n",
    "    def get_token_count(text):\n",
    "        \"\"\"Helper to get the token count (rough approximation).\"\"\"\n",
    "        return len(re.findall(r'\\w+', text))\n",
    "\n",
    "    # Adjust sentence or token counting function\n",
    "    count_func = get_token_count if token_based else len\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        sentences = nltk.sent_tokenize(paragraph.strip())\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "\n",
    "            sentence_size = count_func(sentence)\n",
    "            \n",
    "            # Add sentence to current chunk if it fits\n",
    "            if current_size + sentence_size <= chunk_size:\n",
    "                current_chunk.append(sentence)\n",
    "                current_size += sentence_size\n",
    "            else:\n",
    "                # Finalize the current chunk\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                \n",
    "                # Begin new chunk, optionally with overlap from last few sentences\n",
    "                if overlap > 0 and len(current_chunk) > 0:\n",
    "                    overlap_sentences = current_chunk[-overlap:]\n",
    "                    current_chunk = overlap_sentences + [sentence]\n",
    "                    current_size = count_func(' '.join(current_chunk))\n",
    "                else:\n",
    "                    current_chunk = [sentence]\n",
    "                    current_size = sentence_size\n",
    "\n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "# import google.generativeai as gen_ai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import psycopg2\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "DB_HOST = os.environ['DB_HOST']\n",
    "DB_PORT = os.environ['DB_PORT']\n",
    "DB_NAME = os.environ['DB_NAME']\n",
    "DB_USER = os.environ['DB_USER']\n",
    "DB_PASSWORD = os.environ['DB_PASSWORD']\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=DB_HOST,\n",
    "    port=DB_PORT,\n",
    "    dbname=DB_NAME,\n",
    "    user=DB_USER,\n",
    "    password=DB_PASSWORD\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "def create_embedding_table():\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS document_embeddings (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        document_text TEXT,\n",
    "        embedding VECTOR(768),\n",
    "        source_file TEXT,\n",
    "        chunk_number INT\n",
    "    );\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    " \n",
    "create_embedding_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_pgvector(collection_name, chunks, source_file):\n",
    "    embeddings = [model.encode(chunk).tolist() for chunk in chunks]\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        cursor.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO document_embeddings (document_text, embedding, source_file, chunk_number)\n",
    "            VALUES (%s, %s, %s, %s)\n",
    "            \"\"\",\n",
    "            (chunks[i], embedding, source_file, i)\n",
    "        )\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_index_document(file_path):\n",
    "    text = read_document(file_path)\n",
    "    # text = clean_text(text)\n",
    "    chunks = split_text(text)\n",
    "    add_to_pgvector(\"document_collection\", chunks, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_documents_from_folder(folder_path):\n",
    "    files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(('.pdf', '.txt', '.docx'))]\n",
    "    for file_path in files:\n",
    "        print(f\"Processing {os.path.basename(file_path)}...\")\n",
    "        process_and_index_document(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, n_results=2, similarity_threshold=0.7):\n",
    "    query_embedding = model.encode(query).tolist()\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        SELECT document_text, source_file, chunk_number,\n",
    "               (embedding <=> %s::vector(768)) AS similarity\n",
    "        FROM document_embeddings\n",
    "        ORDER BY similarity ASC\n",
    "        LIMIT %s;\n",
    "        \"\"\",\n",
    "        (query_embedding, n_results)\n",
    "    )\n",
    "    results = cursor.fetchall()\n",
    "    relevant_results = [\n",
    "        {\"text\": result[0], \"source\": result[1], \"chunk\": result[2], \"similarity\": result[3]}\n",
    "        for result in results if result[3] <= similarity_threshold\n",
    "    ]\n",
    "    return relevant_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_with_sources(results):\n",
    "    context = \"\\n\\n\".join([result['text'] for result in results])\n",
    "    sources = [f\"{result['source']} (chunk {result['chunk']})\" for result in results]\n",
    "    return context, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_response(query, response, sources):\n",
    "    formatted_sources = \"\\n\".join([f\"- {source}\" for source in sources])\n",
    "    return f\"\"\"Query: {query}\\n\\nAnswer: {response}\\n\\nSources used:\\n{formatted_sources}\\n\"\"\"\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as gen_ai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "gen_ai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context, conversation_history=\"\"):\n",
    "    prompt = f\"\"\"Based on the following context and conversation history,\n",
    "    please provide a relevant and contextual response. If the answer cannot\n",
    "    be derived from the context, only use the conversation history or say\n",
    "    \"I cannot answer this based on the provided information.\"\n",
    " \n",
    "    Context from documents:\n",
    "    {context}\n",
    " \n",
    "    Previous conversation:\n",
    "    {conversation_history}\n",
    " \n",
    "    Human: {query}\n",
    " \n",
    "    Assistant:\"\"\"\n",
    "   \n",
    "    try:\n",
    "        model = gen_ai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=gen_ai.types.GenerationConfig(\n",
    "                candidate_count=1,\n",
    "                # stop_sequences=[\"x\"],\n",
    "                max_output_tokens=900,\n",
    "                temperature=0.2\n",
    "            ),\n",
    "        )\n",
    "        if response and response.text:\n",
    "            return response.text\n",
    "        else:\n",
    "            return \"Unable to generate content due to response restrictions or empty result.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error generating response: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(query, n_chunks=2):\n",
    "    results = semantic_search(query, n_chunks)\n",
    "    context, sources = get_context_with_sources(results)\n",
    "    response = generate_response(query, context)\n",
    "    return format_response(query, response, sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multiple_queries(queries, n_chunks=2):\n",
    "    responses = {}\n",
    "    for i, query in enumerate(queries):\n",
    "        formatted_response = rag_query(query, n_chunks)\n",
    "        responses[f\"Question {i+1}\"] = formatted_response\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adding document embeddings in postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"E:\\Coding\\python/rag\\docs\"\n",
    "add_documents_from_folder(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are characteristics of cloud computing?\n",
      "\n",
      "Answer: The five essential characteristics of cloud computing are: \n",
      "\n",
      "\n",
      "Sources used:\n",
      "- E:\\Coding\\python/rag\\docs\\2180712_CIS_GTU_Study_Material_e-Notes_All-Units_17062020050424AM.pdf (chunk 3)\n",
      "- E:\\Coding\\python/rag\\docs\\2180712_CIS_GTU_Study_Material_e-Notes_All-Units_17062020050424AM.pdf (chunk 1)\n",
      "\n",
      "Query: Explain arrays in detail.\n",
      "\n",
      "Answer: Arrays are a powerful data structure in programming that allow you to store a collection of elements of the same data type. Think of them like a list or a container that holds multiple values. \n",
      "\n",
      "Here's a breakdown of key concepts:\n",
      "\n",
      "**1. Declaration:**\n",
      "   - You declare an array by specifying its data type and the number of elements it can hold. For example: `int numbers[5];`  This creates an array named `numbers` that can store 5 integer values.\n",
      "\n",
      "**2. Indexing:**\n",
      "   - Each element in an array has a unique index, starting from 0. So, in the `numbers` array above, the first element is at index 0, the second at index 1, and so on.\n",
      "\n",
      "**3. Accessing Elements:**\n",
      "   - You can access individual elements using their index within square brackets. For example, `numbers[0]` would give you the value of the first element in the `numbers` array.\n",
      "\n",
      "**4. Advantages:**\n",
      "   - **Organization:** Arrays help you organize related data in a structured way.\n",
      "   - **Efficiency:**  They allow you to access and manipulate data quickly using indices.\n",
      "   - **Reusability:** You can easily work with large sets of data by using arrays.\n",
      "\n",
      "**Example:**\n",
      "Imagine you want to store the ages of 10 students. You could use an array:\n",
      "\n",
      "```c++\n",
      "int studentAges[10]; // Declare an array to hold 10 integer values\n",
      "\n",
      "// Assign ages to the array\n",
      "studentAges[0] = 18;\n",
      "studentAges[1] = 19;\n",
      "studentAges[2] = 20;\n",
      "// ... and so on\n",
      "\n",
      "// Access the age of the second student\n",
      "int ageOfSecondStudent = studentAges[1]; // This would be 19\n",
      "```\n",
      "\n",
      "**Key Points:**\n",
      "- Arrays are fixed-size. Once you declare an array with a specific size, you cannot change it.\n",
      "- The index always starts from 0.\n",
      "- You can use loops to iterate through all the elements of an array.\n",
      "\n",
      "Let me know if you have any more questions about arrays or want to see more examples! \n",
      "\n",
      "\n",
      "Sources used:\n",
      "- E:\\Coding\\python/rag\\docs\\3140705_OOP---I_GTU_Study_Material_e-Notes_Unit-1-to-5_11062022015400PM (1).pdf (chunk 109)\n",
      "- E:\\Coding\\python/rag\\docs\\3140705_OOP---I_GTU_Study_Material_e-Notes_Unit-1-to-5_11062022015400PM (1).pdf (chunk 112)\n",
      "\n",
      "Query: Explain the benefits of CloudTrail.\n",
      "\n",
      "Answer: CloudTrail offers several benefits, including:\n",
      "\n",
      "* **Simplified Compliance:** CloudTrail automatically records and stores event logs for actions taken within your AWS account, making it easier to conduct compliance audits. \n",
      "* **Detailed Activity Tracking:** It provides a comprehensive audit trail of API calls made by services within your account, giving you visibility into who did what and when. \n",
      "* **Security Monitoring:** You can use CloudTrail logs to monitor for suspicious activity and potential security threats within your AWS environment. \n",
      "* **Troubleshooting:** CloudTrail logs can help you troubleshoot issues by providing a detailed record of events that occurred leading up to a problem. \n",
      "\n",
      "\n",
      "Sources used:\n",
      "- E:\\Coding\\python/rag\\docs\\2180712_CIS_GTU_Study_Material_e-Notes_All-Units_17062020050424AM.pdf (chunk 460)\n",
      "- E:\\Coding\\python/rag\\docs\\2180712_CIS_GTU_Study_Material_e-Notes_All-Units_17062020050424AM.pdf (chunk 160)\n",
      "\n",
      "Query: Describe Amazon Simple Storage Service.\n",
      "\n",
      "Answer: Amazon Simple Storage Service (S3) is a web service offered by Amazon that allows you to store and retrieve any amount of data from anywhere on the web. It's designed with a simple interface for ease of use and focuses on robustness and reliability. \n",
      "\n",
      "\n",
      "Sources used:\n",
      "- E:\\Coding\\python/rag\\docs\\2180712_CIS_GTU_Study_Material_e-Notes_All-Units_17062020050424AM.pdf (chunk 200)\n",
      "- E:\\Coding\\python/rag\\docs\\2180712_CIS_GTU_Study_Material_e-Notes_All-Units_17062020050424AM.pdf (chunk 207)\n",
      "\n",
      "Query: what are data structures?\n",
      "\n",
      "Answer: Data structures are a way of organizing data in a computer's memory. They define the logical relationships between individual data elements, considering not only the elements themselves but also how they relate to each other. Think of them as a blueprint for how data is stored and accessed. \n",
      "\n",
      "\n",
      "Sources used:\n",
      "- E:\\Coding\\python/rag\\docs\\e-Notes_PDF_All-Units_24042019090707AM.pdf (chunk 1)\n",
      "- E:\\Coding\\python/rag\\docs\\e-Notes_PDF_All-Units_24042019090707AM.pdf (chunk 0)\n",
      "\n",
      "Query: What is a stack data structure?\n",
      "\n",
      "Answer: A stack is a data structure where elements are added and removed from only one end, called the \"top\".  It follows the Last-In, First-Out (LIFO) principle, meaning the last element added is the first one removed. Think of it like a stack of plates – you add a plate to the top, and when you want to take one off, you take the top one first. \n",
      "\n",
      "\n",
      "Sources used:\n",
      "- E:\\Coding\\python/rag\\docs\\e-Notes_PDF_All-Units_24042019090707AM.pdf (chunk 10)\n",
      "- E:\\Coding\\python/rag\\docs\\e-Notes_PDF_All-Units_24042019090707AM.pdf (chunk 43)\n",
      "\n",
      "Query: what is case processing in pharmacovigilance?\n",
      "\n",
      "Answer: Case processing in pharmacovigilance is the systematic process of handling adverse event (AE) and serious adverse event (SAE) reports. It involves several steps, including:\n",
      "\n",
      "* **Receipt and Triage:** Receiving reports from various sources (spontaneous, literature, regulatory, clinical studies, legal) and classifying them based on seriousness, causality, and reportability.\n",
      "* **Duplicate Detection:** Identifying and merging duplicate reports to avoid redundancy.\n",
      "* **Data Entry:** Recording information from source documents into the safety database, adhering to procedures, product conventions, and SOPs.\n",
      "* **Follow-up:** Identifying and addressing any missing information or inconsistencies.\n",
      "* **Quality Review:** Ensuring data accuracy and completeness, making corrections as needed, and documenting findings for quality tracking.\n",
      "* **Medical Review:** A physician reviews the case, verifies MedDRA coding, product coding, seriousness, labeling, causality, and narrative, reviews outbound follow-up queries, and prepares comprehensive comments for the Market Authorization Holder (MAH). \n",
      "\n",
      "\n",
      "Sources used:\n",
      "- E:\\Coding\\python/rag\\docs\\Services_Proposal Document - Adrta.docx (chunk 11)\n",
      "- E:\\Coding\\python/rag\\docs\\Services_Proposal Document - Adrta.docx (chunk 13)\n",
      "\n",
      "Query: what is the role of Mr. Ramesh patel in the leadership team?\n",
      "\n",
      "Answer: I cannot answer this based on the provided information. The context mentions Deepak Kumar Singh as the Delivery Head and an India-based Project Manager, but it does not mention any information about Mr. Ramesh Patel. \n",
      "\n",
      "\n",
      "Sources used:\n",
      "- E:\\Coding\\python/rag\\docs\\Services_Proposal Document - Adrta.docx (chunk 16)\n",
      "- E:\\Coding\\python/rag\\docs\\Services_Proposal Document - Adrta.docx (chunk 17)\n",
      "\n",
      "Query: what is the budget strategy of adrta?\n",
      "\n",
      "Answer: I cannot answer this based on the provided information. The provided context focuses on the planning and implementation process, as well as meeting schedules, but does not mention anything about ADRTA's budget strategy. \n",
      "\n",
      "\n",
      "Sources used:\n",
      "- E:\\Coding\\python/rag\\docs\\Services_Proposal Document - Adrta.docx (chunk 6)\n",
      "- E:\\Coding\\python/rag\\docs\\Services_Proposal Document - Adrta.docx (chunk 28)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    " \n",
    "queries = [\n",
    "    \"What are characteristics of cloud computing?\",\n",
    "    \"Explain arrays in detail.\",\n",
    "    \"Explain the benefits of CloudTrail.\",\n",
    "    \"Describe Amazon Simple Storage Service.\",\n",
    "    \"what are data structures?\",\n",
    "    \"What is a stack data structure?\",\n",
    "    # \"How has cloud computing evolved in the past decade?\",\n",
    "    \"what is case processing in pharmacovigilance?\",\n",
    "    \"what is the role of Mr. Ramesh patel in the leadership team?\",\n",
    "    \"what is the budget strategy of adrta?\"\n",
    "]\n",
    "responses = process_multiple_queries(queries)\n",
    " \n",
    "# Display formatted results for each question\n",
    "for question, formatted_response in responses.items():\n",
    "    print(formatted_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
